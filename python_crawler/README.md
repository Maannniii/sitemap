Python crawler to crawl websited to create sitemap.  
The crawler crawls the data and stores in MySQL Database. JSON, XML, CSV files can also be generated by specifying the file name when running the crawler. [Refer this for more info](#running).

### Prerequisites:
1. [python3](https://realpython.com/installing-python "Installing python3")
2. [pip3](https://pip.pypa.io/en/stable/installing/ "Installing pip3")
3. [MySQL 5.7.8+](https://dev.mysql.com/doc/mysql-installation-excerpt/5.7/en/ "Installing MySQL")

This project assumes that you already have the above mentioned prerequisites installed.

### Setup
1. Install the required python libraries by running `pip install -r requirements.txt` found under `sitemap_crawler`.
2. Update the database configurations found at `sitemap_crawler/sitemap_crawler/settings.py`

### Running
* Change directory to `sitemap_crawler`
* To view list of available spiders run `scrapy list`
* To run a spider use `scrapy crawl [spider_name]`. 
* To crawl and store the data as json/xml file run `scrapy crawl [spider_name] -o filename.[json|xml|csv]`. This stores data to Database and Respective file as well.

### Format:
```
[
   {
      "url":"https://abc.com", # link that was crawled
      "external_urls":[        # links to other domains
         "https://abc.com/1",
         "https://abc.com/2"
      ],
      "static_urls":[          # links to statuc contents like images,video,audio
         "https://abc.com/s1",
         "https://abc.com/s2"
      ],
      "urls":[                 # links to other page found on the current page
         "https://abc.com/page1",
         "https://abc.com/page2"
      ]
   }
]
```

## Note:  
* The crawler will truncate the data stored in the database each time. comment the lines 18,19,20 at `sitemap_crawler/pipelines.py` to disable it.
* Running the crawler again and again with same file name will append the data to same file. Remove the file if you don't want to append.
